<!DOCTYPE html>
<html lang="en-US" dir="auto" class="no-js">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>People's Computing</title>
  <meta name="description" content="A Game Historian's View of Ludic Ambient Heuristics">
  <meta name="author" content="Jeff Nyman">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="./site/mstile-144x144.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../site/favicon-16x16.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../site/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="192x192" href="../site/android-chrome-192x192.png">
  <link rel="mask-icon" type="image/svg" href="../site/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../site/apple-touch-icon-180x180.png">
  <link rel="stylesheet" href="../styles/site.css">
  <script type="module">
    document.documentElement.classList.replace("no-js", "js");
  </script>
  <script src="../scripts/site.js" defer></script>
</head>
<body>
  <main>
    <article>
      <h1>Keeping People in Computing</h1>

      <p>
        The idea of people being front-and-center of computation, and thus technology, once held sway but has often been in danger from a wider technocracy. Here I want to focus on what "people's computing" in the broad strokes was.
      </p>

      <h2>An Implied Ontology</h2>

      <p>
        The concept of "people's computing" as an implied ontology, rather than a concrete historical era, introduces a specific perspective. Ontologically, it suggests that the categorization of a specific period as "people's computing" is a construct that we apply retrospectively to encapsulate a set of shared values, practices, and attitudes.
      </p>

      <p>
        By acknowledging the constructed nature of this era, I mean to explicitly highlight the interpretive aspect of historical demarcation. It's not a predefined, objectively existing period, but rather a lens through which we choose to view and define a particular slice of time. This approach aligns with the understanding that historical narratives are shaped by the storytellers, and the identification of distinct eras involves a certain level of subjectivity and interpretation.
      </p>

      <p>
        So my emphasis on the implied ontology is hopefully prompting my readers to consider the fluidity of historical categorizations and the importance of recognizing our role in shaping the narrative. What I hope is that this provides a nuanced perspective that encourages a critical reflection on how we conceptualize and frame historical periods, particularly in the realm of technology and computing.
      </p>

      <p>
        My focus on "people's computing" and the shift from a time when individuals were at the forefront of computation to the potential overshadowing by a broader technocracy is, I hope, a compelling and relevant narrative thread. It speaks to the changing dynamics in the relationship between people and technology over time.
      </p>

      <div class="image-grid">
        <div class="image-stack__item image-stack__item--bottom">
          <img src="../images/history/recreational-computing-magazine.jpg" width="671" height="424">
        </div>
        <div class="image-stack__item image-stack__item--top">
          <img src="../images/history/peoples-computers-magazine.jpg" width="398" height="515">
        </div>
      </div>

      <p>
        The notion of "people's computing" &mdash; during the 1965 to 1975 period &mdash; championed by groups like the People's Computer Center and figures like Bob Albrecht, serves as a poignant reminder of a time when the democratization of technology was a driving force. It reflects a period when individuals, particularly students, were actively engaged in computing, using tools like BASIC to demystify and make technology accessible.
      </p>

      <p>
        My intention to delve into what "people's computing" was in broad strokes is meant to suggest an exploration of the ethos and principles that characterized this era. My hope is that this approach not only captures a historical moment but also sets the stage for reflecting on the implications of a potential shift towards a more technocratic perspective.
      </p>

      <h2>The Crucial Intersection</h2>

      <p>
        The reason for much of my Ludic Historian content is that I've done a lot of research into gaming history, particularly around the distinctions of ludology and narratology. As I embarked on a lot of that research, I found an interesting starting point around where computing and education intersected.
      </p>

      <p>
        Here we a young High School student in 1962:
      </p>

      <img class="prop80" src="../images/history/edu-computing-high-school-boy-1962.jpg" width="655" height="548" alt="Photograph of a High School boy in 1962 using computing technology.">

      <p>
        Here we have a set of younger children in 1967:
      </p>

      <img class="prop80" src="../images/history/edu-computing-kids-in-1967.jpg" width="756" height="580" alt="Photograph of young children in a classroom in 1967 using computing technology.">

      <p>
        In the April-June 2014 issue of <em>IEEE Annals of the History of Computing</em> Joy Rankin wrote an article called "Toward a History of Social Computing: Children, Classrooms, Campuses, and Communities" and in there she says:
      </p>

      <blockquote>
        I must underscore the methodological value of studying systems that originated in an educational context. Because many of the project publications were oriented toward readers in education, they often included meticulous details of users' encounters with the terminal, the language, the lessons, the appropriate syntax, and similar issues. These reports documented knowledge and practices that otherwise would have been tacit and unnoticed. Such descriptions are immensely helpful for historians seeking to understand and describe novel computing experiences.
      </blockquote>

      <p>
        I entirely agree! This intersection provided a very fertile ground for the development of computer games and also artificial intelligence, the latter of which was often used with and against the former to determine its viability and actuality.
      </p>

      <p>
        Here, by way of example, we have Arthur Samuel, in 1959, who published an algorithm for a checkers program using artificial intelligence.
      </p>

      <img class="prop80" src="../images/history/arthur-samuel-1959-checkers.jpg" width="500" height="364" alt="Photograph of Arthur Samuel in 1959 with a checkers playing algorithm.">

      <p>
        What I think is very interesting is that while gaming saw a great deal of experimentation because it became democratized &mdash; due to the democratization of computing &mdash; there was less of this with artificial intelligence. The latter was not as democratized and thus the experimentation was confined to researchers or academics.
      </p>

      <p>
        Thus people, in the broad sense, were front-and-center with gaming-as-computation and very much in the background with AI-as-computation. How ludology and narratology influenced that, even though the terms themselves were not yet in use, is something best covered by example. Here, however, I want to dig into a little history and look at how humans and technology intersected at some pivotal moments.
      </p>

      <h2>A Historical Pivot Point</h2>

      <p>
        A very key moment in the microelectronic revolution, which eventually led to computing becoming accessible to many people, was the launch of the Sputnik satellite on 4 October 1957 by the Soviet Union.
      </p>

      <img src="../images/history/sputnik-report.jpg" width="894" height="708" alt="New York Times front page headline on 5 October 1957 announcing the launch of Sputnik I.">

      <p>
        In the United States, the launch of Sputnik provided the impetus for administrators and educators to reconsider American education as a whole.
      </p>

      <p>
        It was felt a "space race" was coming &mdash; in fact, a technology race &mdash; and thus American education had to prepare its youth for that. Better schooling was required, particularly around math and science.
      </p>

      <h2>Computing in Education</h2>

      <p>
        In September 1958, the United States Congress passed the National Defense Education Act.
      </p>

      <img src="../images/history/defense-education-act.jpg" width="466" height="700" alt="National defense education act: Report to accompany H.R. 13247 published 1 January 1958.">

      <p>
        Not exactly riveting reading, I can assure you, but it provided funds to improve teaching, with an emphasis on teaching science and mathematics. The federal government thus made money available for "innovative approaches" to education. A primary focus of innovation in this context was using technology in the classroom.
      </p>

      <p>
        Money supplied by the government, as well as private institutions like the Carnegie and Ford Foundations, made it possible during the early 1960s for schools to experiment with new and innovative educational methods around technology.
      </p>

      <p>
        This impetus for adding technology into schools, not surprisingly, had a direct impact on the history of computing.
      </p>

      <h2>A Mythology Formed</h2>

      <p>
        If you investigate this history, you generally encounter a persistent mythology in computing. That mythology is that computing went directly from a type of priesthood or elite model of mainframe computing to the liberation from this by the so-called "homebrew hobbyists."
      </p>

      <p>
        These hobbyists were non-professionals who "worked out of their garage" &mdash; hence the "homebrew" &mdash; and ultimately gave us personal computers, one entry point being the kit-based Altair in 1975 to the 1977 "trinity" of the Commodore PET, Apple II and TRS-80.
      </p>

      <img src="../images/history/trinity-1977.jpg" width="1280" height="597" alt="Picture of three machines that formed the 1977 Trinity of personal computers.">

      <p>
        In fact, usually the Commodore and TRS-80 are left out of the mythology and it's seemingly assumed that Apple was solely responsible for carrying the world into a new golden age.
      </p>

      <p>
        All of this eventually led into the early 1980s with the IBM PC and its various clones. Those developments ultimately led to diversification in the 1990s as the "World Wide Web" became the face of the existing Internet.
      </p>

      <p>
        So the general form of the mythology is that we had corporate/institutional computing and we had personal computing. And the shift between them was dramatic and instituted by relatively few people. And these were "special" people who became "big names" such Bill Gates and Steve Jobs.
      </p>

      <p>
        Yet in between those institutional and personal computing contexts, we had what we might want to call "people's computing." This is a part of the history &mdash; and I would argue a very large part of it &mdash; that often gets entirely dismissed, assuming it's even known about at all.
      </p>

      <p>
        This is the crucible era that I referred to in my <a href="./crucible.html">Computing and Crucible Eras</a> article. There was a decade, from 1965 to 1975, where students, educators, and enthusiasts &mdash; ordinary people and not "big names" &mdash; created personal and social computing before personal computers or the broadly public and accessible Internet became available. At that time, the newly emerging people-focused computing access moved in lockstep with network access via time-shared systems.
      </p>

      <h2>Sharing Time</h2>

      <p>
        Those time-shared systems saw their most vibrant use in the context of educational institutions.
      </p>

      <p>
        What thus happened was the technology context combined with the education context and this meant that primary (K-12) schools and high schools, as well as colleges and universities, became sites of technological innovation during the 1960s and 1970s.
      </p>

      <img src="../images/history/students-and-computing.jpg" width="522" height="748" alt="Students at George Washington High School in 1963.">

      <p>
        Students and educators were the ones who built and used academic computing networks, which were, at this time, facilitated by that new type of computing called time-sharing.
      </p>

      <p>
        Time-sharing was a form of networked computing that relied on computing terminals connected to some central computer via telephone lines. Those terminals were located in the supremely social settings of middle school classrooms, college dorm rooms, and university computing labs.
      </p>

      <p>
        Acting as communal institutions, these schools and universities enabled access to and participation with these systems. And this was supported by eager state governments as well as the National Science Foundation.
      </p>

      <p class="popout">
        If you're interested in details about this history I highly recommend John Markoff's <em>What the Dormouse Said: How the Sixties Counterculture Shaped the Personal Computer Industry</em>. Also worth reading is Joy Lisi Rankin's <em>A People's History of Computing in the United States</em> and Bob Johnstone's <em>Never Mind the Laptops: Kids, Computers, and the Transformation of Learning</em>.
      </p>

      <h2>Shared Working and Shared Work</h2>

      <p>
        Collective access to a social, communal resource meant the possibility of storage on a central computer that all the terminals connected into. This meant that users could share useful and enjoyable programs across the network.
      </p>

      <p>
        The "enjoyable" part was often provided by the use of simulations and games. These were obviously of a lot of interest to young students, regardless of their age or skill level. And these simulations and games, more than anything else, led to a democratization in computing.
      </p>

      <p>
        Keep in mind that, by design, time-sharing networks accommodated multiple users. Multiple users meant more possibilities for cooperation, community and communication. This in turn allowed for a great deal of experimentation, collaboration, innovation, and inspiration.
      </p>

      <p>
        A key focus there is on experimentation. The democratization of computing is what led to that experimentation.
      </p>

      <p>
        Advocates of 1960s and 1970s time-sharing networks often shared the belief that computing, information, and knowledge were becoming increasingly crucial to American economic and social success. That viewpoint was related to another, more all-encompassing viewpoint which was that computing would be essential to what many perceived as an emerging "knowledge society."
      </p>

      <p>
        This idea was considered a bit in <em>Time Magazine</em> from April 1965.
      </p>

      <img src="../images/history/time-magazine-april-1965.jpg" width="400" height="527" alt="Time Magazine cover from January April 1965.">

      <p>
        Such a society would certainly be predicated on the sharing of knowledge and thus computing would have to be an essential part not just of storing that knowledge but also making sure it was appropriately democratized. Fostering those viewpoints, and those practices, was certainly another way that the technology and education contexts were aligning because it was perceived that the upcoming generation would be the one to capitalize on these grand visions.
      </p>

      <p class="popout">
        It's perhaps worth noting that artificial intelligence was not really part of this overall focus at all. It's not that it wasn't known about necessarily; but rather its application and use to a wider public was barely considered. A good book that traces a lot of thought on the subject via numerous essays is <em>AI Narratives: A History of Imaginative Thinking about Intelligent Machines</em>.
      </p>

      <p>
        It was in this historical context that people began discussing the possibility of a national computing network. This would be a network that was comparable to the national telephone network or the national electrical grid.
      </p>

      <h3>Brief History of National Networks</h3>

      <p>
        I want to very briefly look at the context for those national service because their context helps undrestand how computing could be aligned with such an idea.
      </p>

      <p>
        The development of the national telephone network in the United States has an interesting historical backdrop. The first telephone exchange was established in New Haven, Connecticut, in 1878, but the real growth started in the 1880s. By the late nineteenth century, major cities were interconnected, and the long-distance network began to take shape.
      </p>

      <p>
        In 1913, the first transcontinental telephone call was made, marking a significant milestone. However, it wasn't until the mid-twentieth century that a truly nationwide network emerged with the advent of direct long-distance dialing in the 1950s.
      </p>

      <p>
        The development of the national electrical grid in the United States is another fascinating historical journey. The initial electric power systems emerged in the late nineteenth century. Thomas Edison's Pearl Street Station in New York City, operational in 1882, is often considered one of the first central power stations.
      </p>

      <p>
        The idea of interconnected power systems began to take shape in the early twentieth century. The Federal Power Act of 1920 provided a legal foundation for the regulation of interstate electricity transmission. The Tennessee Valley Authority, established in 1933 as part of the New Deal encated by President Franklin Roosevelt, was a significant step toward regional electrification.
      </p>

      <p>
        It wasn't until the 1950s and 1960s that the modern national electrical grid started to form. The development of high-voltage transmission lines and the interconnection of regional grids is largely what faciliated this.
      </p>

      <p>
        So what we see here is that by the mid-20th century, the United States had a more cohesive and interconnected electrical grid, ensuring a reliable supply of electricity across the nation, and a well-developed national telephone system, facilitating communication on a broad scale. These infrastructural developments were crucial for the socio-economic growth and technological advancement of the country during that period.
      </p>

      <p>
        It's apt to describe this timeframe as a period of the commoditization of power and communication. Both electricity and telephone services became increasingly standardized, accessible, and essential for individuals and businesses alike. The widespread availability and reliance on these services contributed to a sense of them being commodities &mdash; essential, widely available goods that form the backbone of modern life. This was the context in which computing would eventually be considered.
      </p>

      <h2>Computing as Commodity</h2>

      <p>
        During the 1960s, those focused on academics and those focused on business both grew increasingly interested in &mdash; and evangelical about &mdash; this idea of a national computing utility. Perhaps even multiple such computing utilities. The idea here was that computing services would be delivered across the United States over time-sharing networks.
      </p>

      <p>
        Entire businesses were launched to realize this particular vision. That vision was a world in which all Americans benefited from computing access in their homes and workplaces, just as Americans benefited from the comparable national utilities of water, electricity, and telephone service.
      </p>

      <p>
        Technology had to enable all of these grand visions, of course, and, for a time, there was a symbiotic relationship with the minicomputer marketplace, which provided a lower cost alternative to very expensive and very large mainframes. Those mainframes were computing solutions that schools often couldn't afford.
      </p>

      <p>
        It was minicomputers that the terminals at various schools were connecting into with their various access methods, such as teletype terminals.
      </p>

      <img class="mask" src="../images/history/minicomputer-connections.jpg" width="600" height="448" alt="Dartmouth students at teletype terminals connected to minicomputers.">

      <p>
        Minicomputer manufacturers, like the Digital Equipment Corporation and Hewlett Packard, monetized their support of educational materials to sell their machines, further aligning the technological with educational interests.
      </p>

      <h2>A Computing Ecosystem Formed</h2>

      <p>
        What I hope you can see is that the histories of educational institutions and the technology companies are woven together but still distinct. They certainly had areas where their interests aligned but also, of course, had very specific interests of their own that they worked toward.
      </p>

      <p>
        It was the educational aspects, however, that would give rise to the idea of people's computing, with the technology companies figuring out not only how to be enablers of that but how to make as much money as they could doing so.
      </p>

      <p>
        The early, more localized networks collectively embodied the desire for computer resource sharing. In an education context, this immediately situated itself around a community of interested individuals joined by computing networks. The desire was for a form of communal computing which could be extended beyond the education context.
      </p>

      <p>
        That "beyond the education context" impetus is what propelled the push for the national computing utility that I mentioned earlier and the promise of a nation of computing citizens. This further allowed for the idea of computing being brought into the home and thus becoming more "personal."
      </p>

      <h2>A Broad Focus for Computing</h2>

      <p>
        So let's consider the broad sweep of the history being discussed here as it developed.
      </p>

      <p>
        We ended up with educational computing enthusiasts like Noble Gividen, Bob Albrecht and David Ahl who did a massive amount of work around introducing computing to young students. Various computing projects within education started, such as Project SOLO, Project LOCAL and the Huntington Project.
      </p>

      <img src="../images/history/education-projects.jpg" width="1154" height="273" alt="Logos for the Projet SOLO, Projet LOCAL, and Huntington Project computing initiatives.">

      <p>
        Those projects allowed for the work of the enthusiasts, and their students, to be shared in various newsletters and periodicals.
      </p>

      <p>
        There were also education initiatives such as the Board of Cooperative Educational Services (BOCES), Total Information for Educational Systems (TIES) and the Minnesota Educational Computing Consortium (MECC).
      </p>

      <p class="popout">
        For those who know their gaming history, BOCES is what led to <em>The Sumerian Game</em> which ultimately provided the basis for all <em>Hammurabi</em> variants. TIES and MECC are the contexts in which <em>The Oregon Trail</em> proliferated.
      </p>

      <p>
        As all of this democratization and experimentation occurred, there were people who began to build <em>hobbies</em> around the new technology and, eventually, there were people who began to build <em>careers</em> around the new technology. That gradual shift could not have happened without the experimentation that was enabled by democratization mentioned earlier.
      </p>

      <p>
        All of this forged a crucial link between the computer &mdash; and thus computing &mdash; and its larger social and economic environment. This, in turn, led to different technical communities and distinctive subcultures and thus varying ecosystems. And that, in turn, led to discussions about the relationship between science and craft in engineering practice, both of hardware and software.
      </p>

      <p>
        Those communities, subcultures and discussions eventually led to hardware being a consumer technology and to software being a commercial industry.
      </p>

      <p>
        We thus eventually saw the rise of an information economy serving as the backbone of an information society. <em>Time Magazine</em>, in its February 1978 issue, certainly recognized that.
      </p>

      <img src="../images/history/time-magazine-february-1978.jpg" width="400" height="527" alt="Time Magazine cover from February 1978.">

      <p>
        Yet notice how the focus shifted. We were apparently a "computer society" versus the "knowledge society" that was originally envisioned. The technocracy had already started to shift the dynamic.
      </p>

      <p>
        In this "computer society," games and gaming have proved to be a staggering multi-billion dollar industry, one that, at the time of writing this, easily outpaces all other forms of entertainment. And, to be sure, there were various forays into artificial intelligence, most of which led to so-called "AI winters."
      </p>

      <p class="popout">
        The concept of AI winters gained prominence after the initial excitement and high expectations in the late 1950s and early 1960s, followed by periods of reduced enthusiasm and funding due to challenges and limitations in research around artificial intelligence. The first AI winter occurred in the 1970s and another, more prolonged one, in the late 1980s to the early 1990s.
      </p>

      <p>
        Yet here we are now, as I write this, seeing yet another major foray into artificial intelligence and its broad dissemination across an array of industries. That means we are seeing a lot more experimentation and thus the possible democratization I mentioned previously. The intersection of artificial intelligence with gaming and simulations has accelerated once again.
      </p>

      <h2>The Historical Broad Stroke</h2>

      <p>
        So here's where my initial historical forays led me: in the late 1950s, computers were, for the most part, remote, inaccessible, and unfamiliar to pretty much everyone.
      </p>

      <p>
        Speaking just to the United States, there were approximately six thousand computer installations and those tended to cluster in military, business, and research-focused university settings. Computing was focused on the academic, industrial, and military. Individual access to computing was extremely rare.
      </p>

      <p>
        As per the broad history I recounted above, computers would start to spread from military and university installations through factories and offices and, eventually, into the home.
      </p>

      <img src="../images/history/computing-evolution.jpg" width="529" height="396" alt="Visual showing computing evolution from mainframe to minicomputer to personal computer.">

      <p>
        But all that took a little bit of time. And in between that time &mdash; between, that is, institutional/corporate computing and personal computing &mdash; there was a fascinating emergence of people's computing. And a whole lot of that people's computing was driven by a focus on simulation and games and, to a lesser extent, artificial intelligence.
      </p>

      <p>
        Nathan Ensmenger wrote an article called "Power to the People: Toward a Social History of Computing" in the January-March 2004 issue of <em>IEEE Annals of the History of Computing</em> where he said:
      </p>

      <blockquote>
        The future of the history of computing is not machines, but people.
      </blockquote>

      <p>
        Indeed, and the same can be said of the past.
      </p>

      <p>
        In the April-June 2014 issue of that same journal, Joy Rankin, in her article "Toward a History of Social Computing: Children, Classrooms, Campuses, and Communities" said:
      </p>

      <blockquote>
        We historians have only begun to address how people made computing ubiquitous.
      </blockquote>

      <p>
        Exactly right. And that's a focus I want to continue on.
      </p>
    </article>
  </main>
</body>
</html>
