<!DOCTYPE html>
<html lang="en-US" dir="auto" class="no-js">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Computing and Crucible Eras</title>
  <meta name="description" content="A Game Historian's View of Ludic Ambient Heuristics">
  <meta name="author" content="Jeff Nyman">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="./site/mstile-144x144.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../site/favicon-16x16.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../site/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="192x192" href="../site/android-chrome-192x192.png">
  <link rel="mask-icon" type="image/svg" href="../site/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../site/apple-touch-icon-180x180.png">
  <link rel="stylesheet" href="../styles/site.css">
  <script type="module">
    document.documentElement.classList.replace("no-js", "js");
  </script>
  <script src="../scripts/site.js" defer></script>
</head>
<body>
  <main>
    <article>
      <h1>Computing and Crucible Eras</h1>

      <hr>

      <p>
        The focus in this article is not on any specific games but rather on the broader historical context in which games, and gaming, came to be situated in a technology context, specifically that of computing.
      </p>

      <h2>A Basis in Experimentation ...</h2>

      <p>
        Exploring the early days of computing and game development is like peering into the wild west of technology. In the nascent stages, hobbyists &mdash; armed with programming languages like BASIC &mdash; were somewhat similar to pioneers in uncharted territory.
      </p>

      <p>
        These early game developers weren't just coding; they were conducting experiments, pushing the limits of the available technology to see what it could do.
      </p>

      <img src="../images/history/games-experiments.jpg" width="467" height="203" alt="Visual showing the words 'Games' and 'Experiments'.">

      <p>
        It was a fusion of creativity and curiosity, where every line of code was a step into the unknown. Much like scientists in a lab, they were learning by doing, discovering the capabilities and constraints of the hardware as they went along.
      </p>

      <p>
        Crucially, these games weren't just entertainment; they were manifestations of the evolving relationship between humans and machines, each line of code a small triumph in the ongoing saga of technological discovery. It was a time when the boundaries of possibility were drawn by the imaginations of those who dared to experiment with the digital frontier.
      </p>

      <h2>... But a Focus on Computation</h2>

      <p>
        Arguably <em>computer</em> science should have been called <em>computing</em> science. Or, at the very least, perhaps there should have been a shift to that terminology as the technology evolved more beyond the hardware itself and into what the software that was running on that hardware actually did.
      </p>

      <img src="../images/history/computation.jpg" width="570" height="391" alt="Stylized visual of the idea of computation.">

      <p>
        When I say "computation," please understand that this isn't just about the mechanics of programming or coding. It's about computational design, the latest incarnation of which is most manifested in applications of artificial intelligence.
      </p>

      <p class="popout">
        As a matter of interest, artificial intelligence actually grew up with the idea of technology-based gaming and continues to be used in that context today.
      </p>

      <p>
        Computation has always been, and still is, what transforms the design of our products and services. This is the intersection of humans and technology, an area that's always fascinated me. Design has always determined how humanizing or dehumanizing that intersection is. One of the ways that computing was always most humanized was when it focused on simulations and games.
      </p>

      <h2>When Computing Was Human</h2>

      <p>
        It's probably worth keeping in mind that the first computers weren't machines. They were instead human beings. They were humans who computed, by which was meant "worked with numbers."
      </p>

      <img src="../images/history/human-computers.jpg" width="595" height="742" alt="Old photograph of a woman doing computations, acting as a human computer.">

      <p>
        Consider that way back in 1613 a poet named Richard Brathwait, in his <em>The Yong Mans Gleanings</em>, said:
      </p>

      <blockquote>
        What art thou (O Man) and from whence hadst thou thy beginning? What matter art thou made of, that thou promisest to thy selfe length of daies: or to thy posterity continuance. I haue read the truest computer of Times, and the best Arithmetician that euer breathed, and he reduceth thy dayes into a short number: The daies of Man are threescore and ten.
      </blockquote>

      <p>
        Did you spot that interesting word in there? <em>Computer</em>. I'll say again: <em>this passage was from 1613</em>.
      </p>

      <p>
        So who is this "best Arithmetician" that Brathwait is referring to here? Some have argued that he was conceiving of the "computer of Times" as a divine being that would be able to calculate the exact length of a person's life. Others suggest that Brathwait was legitimately referring to a person who's very good at arithmetic.
      </p>

      <p>
        As an interesting related note, the mathematician Gottfried Leibniz said this:
      </p>

      <blockquote>
        If controversies were to arise, there would be no more need of disputation between two philosophers than between two calculators. For it would suffice them to take their pencils in their hands and to sit down at the abacus, and to say to each other . . . Let us calculate.
      </blockquote>

      <p>
        That was in his 1685 book <em>The Art of Discovery</em>. Here when he says "calculators" he's referring to people and "calculate" as an action they do. Note that Leibniz didn't use the term "computers" or "compute." This makes it all the more interesting that Brathwait seemingly did hit upon that term.
      </p>

      <p>
        As far as we know, Brathwait is the first to actually write the word "computer" even if we're not quite sure what he meant by it. According to the Oxford English Dictionary there was an earlier usage. Apparently the term "computer" was first used verbally back in 1579. No details are provided and so, at least to my knowledge, there's no way to actually corroborate that.
      </p>

      <p>
        That said, the reference in the Oxford English Dictionary links the verbal use of the term from 1579 to "arithmetical or mathematical reckoning." And this certainly was a task done by people. This is referenced by Sir Thomas Browne in volume six of <em>Pseudodoxia Epidemica</em> from 1646 as well as Jonathan Swift in <em>A Tale of a Tub</em> from 1704.
      </p>

      <p>
        So while we don't have a direct thread linking Brathwait's use of the term to those later usages, it's clear that the general idea of "computing," even if going by another name, was in the general Zeitgeist and was focused on what we would certainly now call "computing."
      </p>

      <p>
        In 1895, the Century Dictionary defined a "computer" as:
      </p>

      <blockquote>
        One who computes; a reckoner; a calculator.
      </blockquote>

      <p class="feature">
        If we take a side trip into etymology, the root <em>com</em> originates from Latin, meaning "together." The suffix <em>puter</em> likewise has its basis in the Latin word <em>putare</em> which means "to think or trim." Some have proposed that the idea of computer thus meant a "setting to rights" or a "reckoning up."
      </p>

      <p>
        Eventually computing machines came along to replace the human computers. A relevant idea that really resonated with me comes from John Maeda in his book <em>How To Speak Machine: Computational Thinking for the Rest of Us</em>:
      </p>

      <blockquote>
        To remain connected to the humanity that can easily be rendered invisible when typing away, expressionless, in front of a metallic box, I try to keep in mind the many people who first served the role of computing 'machinery' . . . It reminds us of the intrinsically human past we share with the machines of today.
      </blockquote>

      <p>
        Indeed! And as we try to build computing solutions that "act more like us" or "think more like us" or "learn more like us," I think that Maeda's reminder takes on an interesting focus. And that focus of a human past we share with our technology becomes very interesting in the context of games.
      </p>
    </article>
  </main>
</body>
</html>
